{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "id": "b8E7EukT4r44"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn.preprocessing as skl_pre\n",
        "import sklearn.linear_model as skl_lm\n",
        "import sklearn.discriminant_analysis as skl_da\n",
        "import sklearn.neighbors as skl_nb\n",
        "\n",
        "#from IPython.display import set_matplotlib_formats\n",
        "#set_matplotlib_formats('png')\n",
        "from IPython.core.pylabtools import figsize\n",
        "figsize(10, 6) # Width and hight\n",
        "#plt.style.use('seaborn-white')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "T76o6_5v4r47"
      },
      "source": [
        "# 4.1 Getting started with classification – Breast cancer diagnosis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "Qn4W55U84r49"
      },
      "source": [
        "In this exercise, we will consider the data set `data/biopsy.csv` with data from breast biopsies, for the purpose of diagnosing breast cancer. For each patient, the data set contains nine different attributes (clump thickness, uniformity of cell size, uniformity of cell shape, marginal adhesion, single epithelial cell size, bare nuclei, bland chromatin, normal nucleoli and mitoses) scored on a scale from $1$ to $10$, as well as the physician’s diagnosis (malign or benign)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "VzeLQ6lB4r4-"
      },
      "source": [
        "## Dataset\n",
        "This data frame `biopsy` contains the following columns:  \n",
        "`ID`: sample code number (not unique).  \n",
        "`V1`: clump thickness.  \n",
        "`V2`: uniformity of cell size.  \n",
        "`V3`: uniformity of cell shape.  \n",
        "`V4`: marginal adhesion.  \n",
        "`V5`: single epithelial cell size.  \n",
        "`V6`: bare nuclei (16 values are missing).  \n",
        "`V7`: bland chromatin.  \n",
        "`V8`: normal nucleoli.  \n",
        "`V9`: mitoses.  \n",
        "`class`: \"benign\" or \"malignant\".  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "CR5Al3VY4r4-"
      },
      "source": [
        "## a)\n",
        "Load and familiarize yourself with the data set, using, e.g.`info()`, `describe()`, `pandas.plotting.scatter_matrix()` and `print()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "xC0iJtG04r4_"
      },
      "outputs": [],
      "source": [
        "# url = 'data/biopsy.csv'\n",
        "url = 'https://uu-sml.github.io/course-sml-public/data/biopsy.csv'\n",
        "biopsy = pd.read_csv(url, na_values='?', dtype={'ID': str}).dropna().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "0_bapMwT4r4_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "lRIoY8Cn4r5A"
      },
      "source": [
        "## b)\n",
        "Split the data randomly into a training set and a test set of approximately similar size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": [],
        "id": "QF19NkM_4r5A"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1)\n",
        "trainI = np.random.choice(biopsy.shape[0], size=300, replace=False)\n",
        "trainIndex =biopsy.index.isin(trainI)\n",
        "train = biopsy[trainIndex]\n",
        "test = biopsy[~trainIndex]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "28y_PWVH4r5B"
      },
      "source": [
        "## c) Logistic regression <a id='4.1-c'></a>\n",
        "Perform logistic regression with `class` as output variable and `V3`, `V4` and `V5` as input variables. Do a prediction on the test set, and compute (i) the fraction of correct predictions and (ii) the confusion matrix (using, for examnple, `pandas.crosstab()`). The commands `skl_lm.LogisticRegression()` and `model.predict()` are useful. Is the performance any good, and what does the confusion matrix tell you?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "tags": [],
        "id": "g-kX6Td84r5B",
        "outputId": "b0809bc6-be30-4f6c-f1ca-ee2c7467b1e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Summary\n",
            "LogisticRegression()\n"
          ]
        }
      ],
      "source": [
        "model = skl_lm.LogisticRegression(solver='lbfgs')\n",
        "X_train = train[['V3', 'V4', 'V5']]\n",
        "Y_train = train['class']\n",
        "X_test = train[['V3', 'V4', 'V5']]\n",
        "Y_test = train['class']\n",
        "model.fit(X_train,Y_train)\n",
        "print(\"Model Summary\")\n",
        "print(model)\n",
        "\n",
        "predic_prob= model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "Vpq3fl5f4r5B"
      },
      "source": [
        "## d) LDA\n",
        "Repeat [(c)](#4.1-c) using LDA. A useful command is `sklearn.discriminant_analysis.LinearDiscriminantAnalysis()`. `sklearn.discriminant_analysis` is imported as `skl_da`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "UVSDsiY84r5B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "OTYA-tvl4r5C"
      },
      "source": [
        "## e) QDA\n",
        "Repeat [(c)](#4.1-c) using QDA. A useful command is `sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NEH5Jwsk4r5E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "6JH_x-1S4r5E"
      },
      "source": [
        "## f) KNN\n",
        "Repeat [(c)](#4.1-c) using $k$-NN (with $k = 1$). A useful commands is `sklearn.neighbors.KNeighborsClassifier()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "zwvKWZsh4r5E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ESmKKwZs4r5E"
      },
      "source": [
        "## g) Try different values for KNN\n",
        "Use a `for`-loop to explore the performance of $k$-NN for different values of $k$, and plot the fraction of correct\n",
        "predictions as a function of $k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "GtM7vhYD4r5E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "HymMjuim4r5E"
      },
      "source": [
        "## h) ROC for logistic regression\n",
        "Use a `for`-loop to explore how the true and false positive rates in logistic regression are affected by different threshold values, and plot the result as a `ROC curve`. (see Figure 4.7 and Table 4.1 in the book)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "3qTUwO2D4r5E"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "zkSUApDQ4r5E"
      },
      "source": [
        "## i)\n",
        "Try to find another set of inputs (perhaps by also considering transformations of the attributes) which gives a better result than you have achieved so far. You may also play with the threshold values. (“Better” is on purpose left vague. For this problem, the implications of a false negative (=`benign`) misclassification is probably more severe than a false positive (=`malignant`) misclassification.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "vZAWb_zz4r5F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "JyAJ0pg54r5F"
      },
      "source": [
        "# 4.2 Decision boundaries\n",
        "The following code generates some data with $x_1$ and $x_2$ both in $[0, 10]$ and $y$ either $0$ or $1$, and plots the decision boundary for a logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "tfC2jvig4r5F"
      },
      "outputs": [],
      "source": [
        "# generate data\n",
        "np.random.seed(2)\n",
        "N = 100\n",
        "x1 = np.random.uniform(0, 10, N)\n",
        "x2 = np.random.uniform(0, 10, N)\n",
        "y = np.ones(N)\n",
        "y[x1<4] = 0\n",
        "y[x2<4] = 0\n",
        "X = pd.DataFrame({'x1': x1, 'x2': x2})\n",
        "\n",
        "# learn a logistic regression model\n",
        "model = skl_lm.LogisticRegression(solver='lbfgs')\n",
        "model.fit(X, y)\n",
        "\n",
        "# classify the points in the whole domain\n",
        "res = 0.01   # resolution of the squares\n",
        "xs1 = np.arange(0, 10 + res, res)\n",
        "xs2 = np.arange(0, 10 + res, res)\n",
        "xs1, xs2 = np.meshgrid(xs1, xs2)    # Creating the grid for all the data points\n",
        "X_all = pd.DataFrame({'x1': xs1.flatten(), 'x2': xs2.flatten()})\n",
        "prediction = model.predict(X_all)\n",
        "\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "mycmap = LinearSegmentedColormap.from_list('mycmap', ['skyblue', 'lightsalmon'])\n",
        "plt.imshow(prediction.reshape(xs1.shape[0],-1),\n",
        "           cmap=mycmap,\n",
        "           origin='lower',\n",
        "           extent=[0,10,0,10],\n",
        "           aspect='auto')\n",
        "\n",
        "# Plot of the data points and their label\n",
        "plt.scatter(x1, x2, c=y, cmap='bwr') # blue - white -red colormap\n",
        "\n",
        "plt.title('Logistic regression decision boundary')\n",
        "plt.xlim([0,10])\n",
        "plt.ylim([0,10])\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "r-nIxIW94r5F"
      },
      "source": [
        "## (a)\n",
        "Run the code and verify that it reproduces the figure, and make sure you understand the figure. What is the misclassification rate here?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "rVdK3Ix14r5F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "HZFfdxX94r5F"
      },
      "source": [
        "## (b)\n",
        "Modify the code to plot the decision boundary for a LDA classifier. What differences do you see? What is the misclassification rate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "7cyqFSPC4r5F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "45i7KuWH4r5G"
      },
      "source": [
        "## (c)\n",
        "Modify the code to plot the decision boundary for a QDA classifier. What differences do you see? What is the misclassification rate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "IOhAYqgH4r5G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "oTb2oUwZ4r5G"
      },
      "source": [
        "## (d)\n",
        "Modify the code to plot the decision boundary for a $k$-NN classifier. What differences do you see? What is the misclassification rate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "su8t-20u4r5G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "8PtbXQiB4r5G"
      },
      "source": [
        "## (e)\n",
        "What happens with the decision boundary for logistic regression if you include the term $x_1x_2$ as an input? What is the misclassification rate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "KMlJ8gCw4r5G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "HUaDAXDu4r5G"
      },
      "source": [
        "# 4.3 Why not linear regression?\n",
        "In this exercise, we explore why linear regression might not be well suited for classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "LKPvaAl14r5G"
      },
      "source": [
        "## (a)\n",
        "Construct and plot a data set as follows: Let $x_i$ be samples $x_i = i$ in a sequence from $i = 1$ to $i = 40$. Let $y_i = 0$ for all $i = 1 : 40$, except for $i = 34, 38, 39, 40$ where $y_i = 1$. Hence, $y$ belongs to either of two classes, $0$ and $1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "wtvGcQ1A4r5H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "V7G7DT-L4r5H"
      },
      "source": [
        "## (b)\n",
        "Now, the problem is to fit a model which is able to predict the output $y$ from the input $x$. Start with a linear regression model (command `skl_lm.LinearRegression()`), and simply threshold its predictions at 0.5 (the average of 0 and 1, the two classes). Plot the prediction. How good is the prediction?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "clxiokns4r5L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "0xkOfqQm4r5L"
      },
      "source": [
        "## (c)\n",
        "Try instead logistic regression using `skl_lm.LogisticRegression()` command (set the parameter `C` to $1000$) and plot the prediction. How good is the prediction, and what advantages does logistic regression have over linear regression for this classification problem?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "PkO7dVHV4r5L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "kKssxr6H4r5M"
      },
      "source": [
        "# 4.4 k-NN\n",
        "In this exercise, we are going to explore an important user aspect of $k$-NN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "OUU5gjF94r5M"
      },
      "source": [
        "## (a)\n",
        "Make $200$ draws $x_1$ from a $\\mathcal{N}(0, 1^2)$ distribution, and $200$ draws $x_2$ from $\\mathcal{N}(0, 10^4)$. Also construct $y$ such that $y = 1$ if $x_1 \\cdot x_2$ is positive, and $0$ otherwise. Split the data set randomly into a test and a training data set (equally sized)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "KsZYwl7M4r5M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "3KcX-UIc4r5M"
      },
      "source": [
        "## (b)\n",
        "Use $k$-NN (choose $k$ yourself) to predict the test output $y$ using $x_1$ and $x_2$ as inputs. How well do you perform?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TUpTp5TE4r5N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "rQbfvf-Q4r5N"
      },
      "source": [
        "## (c)\n",
        "Now replace $x_2$ with $200$ draws from $\\mathcal{N}(0, 1^2)$, and perform $k$-NN classification anew. How well do you perform this time? Explain the difference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "c-HaXjUT4r5N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "HLDITVMg4r5N"
      },
      "source": [
        "## (d)\n",
        "Explore how the `sklearn.preprocessing.scale()` function can help for such problems encountered in (b)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "7kDXJVDl4r5O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "qW2LB1GL4r5O"
      },
      "source": [
        "# 4.5 Multiclass classification\n",
        "In the course, we have focused on the classification problem for 2 classes. The methods can, however, be generalized to more than two classes. In `Python`, the commands `skl_da.LinearDiscriminantAnalysis()`, `skl_da.QuadraticDiscriminantAnalysis()` and `skl_nb.KNeighborsClassifier()` can all be used directly for multi-class problems as well, which we will do in this exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "vYCDoIRo4r5O"
      },
      "source": [
        "## (a)\n",
        "Load and familiarize yourself with the data set `iris`, and split it randomly into a training and a test data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "cT95urLM4r5O"
      },
      "source": [
        "**Description**\n",
        "\n",
        "This famous (Fisher's or Anderson's) `iris` data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for $50$ flowers from each of $3$ species of iris. The species are Iris setosa, versicolor, and virginica.\n",
        "\n",
        "**Format**\n",
        "\n",
        "iris is a data frame with $150$ cases (rows) and $5$ variables (columns) named `Sepal.Length`, `Sepal.Width`, `Petal.Length`, `Petal.Width`, and `Species`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NXUlvymW4r5P"
      },
      "outputs": [],
      "source": [
        "# url = 'data/iris.csv'\n",
        "url = 'https://uu-sml.github.io/course-sml-public/data/iris.csv'\n",
        "iris = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "XvTU-pgR4r5P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "c4FM3gXo4r5P"
      },
      "source": [
        "## (b)\n",
        "Use all inputs (`Sepal.Length`, `Sepal.Width`, `Petal.Length`, `Petal.Width`) to predict the output `Species` (`setosa`,\n",
        "`versicolor` and `virginica`) using LDA, QDA, and $k$-NN, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "PRWxFECs4r5P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}